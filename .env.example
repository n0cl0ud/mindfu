# =============================================================================
# MindFu Configuration
# =============================================================================
# Copy this file to .env and customize as needed

# -----------------------------------------------------------------------------
# LLM Configuration
# -----------------------------------------------------------------------------
# Model to use for inference (HuggingFace model ID)
MODEL_NAME=mistralai/Devstral-Small-2505

# Maximum context length (tokens)
MAX_MODEL_LEN=8192

# GPU memory utilization (0.0-1.0)
GPU_MEMORY_UTILIZATION=0.90

# Quantization method (awq, gptq, or empty for none)
QUANTIZATION=awq

# HuggingFace token (required for gated models)
HF_TOKEN=

# -----------------------------------------------------------------------------
# vLLM Configuration (profile: vllm)
# Usage: docker compose --profile vllm up -d
# -----------------------------------------------------------------------------
# vLLM image version (latest, v0.6.6, nightly, etc.)
VLLM_VERSION=latest

# Model for vLLM inference
VLLM_MODEL=mistralai/Devstral-Small-2-24B-Instruct-2512

# Maximum context length for vLLM (Devstral supports up to 128K)
VLLM_MAX_MODEL_LEN=81920

# GPU memory utilization (0.0-1.0)
VLLM_GPU_MEM=0.95

# KV cache dtype (auto, fp8, fp8_e5m2, fp8_e4m3)
VLLM_KV_CACHE_DTYPE=fp8

# Tensor parallel size (number of GPUs for model parallelism)
VLLM_TP_SIZE=1

# Attention backend (FLASHINFER, FLASH_ATTN, XFORMERS)
VLLM_ATTENTION_BACKEND=FLASHINFER

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
POSTGRES_USER=mindfu
POSTGRES_PASSWORD=mindfu_secret
POSTGRES_DB=mindfu

# -----------------------------------------------------------------------------
# RAG Configuration
# -----------------------------------------------------------------------------
# Embedding model for document similarity
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Default Qdrant collection name
DEFAULT_COLLECTION=documents

# Enable conversation logging for training
LOG_CONVERSATIONS=true

# Chunk size for document splitting (tokens)
CHUNK_SIZE=512

# Chunk overlap (tokens)
CHUNK_OVERLAP=50

# Number of context chunks to retrieve
TOP_K=5

# -----------------------------------------------------------------------------
# Training Configuration
# -----------------------------------------------------------------------------
# LoRA rank (higher = more capacity, more memory)
LORA_RANK=16

# LoRA alpha
LORA_ALPHA=32

# Training batch size
BATCH_SIZE=4

# Gradient accumulation steps
GRADIENT_ACCUMULATION_STEPS=4

# Learning rate
LEARNING_RATE=2e-4

# Number of training epochs
NUM_EPOCHS=3

# -----------------------------------------------------------------------------
# Optional: External Services
# -----------------------------------------------------------------------------
# Uncomment and set if using external services instead of Docker

# QDRANT_URL=http://localhost:6333
# REDIS_URL=redis://localhost:6379
# POSTGRES_URL=postgresql://mindfu:mindfu_secret@localhost:5432/mindfu
