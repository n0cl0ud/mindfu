# =============================================================================
# MindFu - Local LLM Infrastructure with RAG
# =============================================================================
#
# Usage:
#   GGUF/llama.cpp (RTX 5080, consumer GPUs):
#     docker compose --profile gguf up -d
#
#   vLLM Devstral (L40S, A100, H100 - datacenter GPUs):
#     docker compose --profile vllm up -d
#
#   vLLM Nemotron 3 Nano (L40S, A100, H100 - 1M context):
#     docker compose --profile nemotron up -d
#
#   Training (L40S/A100/H100):
#     docker compose --profile vllm --profile training up -d
#
# Client endpoint: http://localhost:11434 (OpenAI-compatible)
# =============================================================================

services:
  # =============================================================================
  # SHARED INFRASTRUCTURE (always started)
  # =============================================================================

  # ---------------------------------------------------------------------------
  # Vector Database (Qdrant)
  # ---------------------------------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: mindfu-qdrant
    volumes:
      - qdrant-data:/qdrant/storage
      - ./configs/qdrant:/qdrant/config:ro
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=100
    networks:
      - mindfu-network
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # PostgreSQL (Conversation Logs & Metadata) - Only for logging/training
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: mindfu-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-mindfu}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mindfu_secret}
      - POSTGRES_DB=${POSTGRES_DB:-mindfu}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mindfu}"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mindfu-network
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Redis (Caching)
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: mindfu-redis
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - mindfu-network
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # MLflow (Experiment Tracking) - Only for training
  # ---------------------------------------------------------------------------
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.1
    container_name: mindfu-mlflow
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    volumes:
      - mlflow-artifacts:/mlflow
    ports:
      - "5000:5000"
    profiles:
      - training
    networks:
      - mindfu-network
    restart: unless-stopped

  # =============================================================================
  # GGUF STACK (profile: gguf) - For consumer GPUs (RTX 5080, RTX 4090, etc.)
  # =============================================================================

  # ---------------------------------------------------------------------------
  # LLM Inference - llama.cpp (GGUF quantized)
  # ---------------------------------------------------------------------------
  llm:
    image: ghcr.io/n0cl0ud/mindfu-llm:latest
    container_name: mindfu-llm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - MODEL_REPO=${MODEL_REPO:-unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF}
      - MODEL_FILE=${MODEL_FILE:-Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf}
      - N_CTX=${N_CTX:-8192}
      - N_GPU_LAYERS=${N_GPU_LAYERS:--1}
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - model-cache:/root/.cache/huggingface
      - ./data/models:/models
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 600s
    profiles:
      - gguf
    networks:
      - mindfu-network
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # RAG Service for GGUF backend
  # ---------------------------------------------------------------------------
  rag:
    image: ghcr.io/n0cl0ud/mindfu-rag:latest
    container_name: mindfu-rag
    environment:
      - LLM_BASE_URL=http://llm:8000/v1
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-mindfu}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mindfu_secret}
      - POSTGRES_DB=${POSTGRES_DB:-mindfu}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      - DEFAULT_COLLECTION=${DEFAULT_COLLECTION:-documents}
      - LOG_CONVERSATIONS=${LOG_CONVERSATIONS:-false}
    volumes:
      - ./data/documents:/app/documents
      - ./data/conversations:/app/conversations
      - embedding-cache:/root/.cache/huggingface
    ports:
      - "11434:8080"
    depends_on:
      qdrant:
        condition: service_started
      redis:
        condition: service_healthy
      llm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    profiles:
      - gguf
    networks:
      - mindfu-network
    restart: unless-stopped

  # =============================================================================
  # vLLM STACK (profile: vllm) - For datacenter GPUs (L40S, A100, H100)
  # =============================================================================

  # ---------------------------------------------------------------------------
  # LLM Inference - vLLM (full precision with FP8 KV cache)
  # ---------------------------------------------------------------------------
  llm-vllm:
    image: vllm/vllm-openai:${VLLM_VERSION:-nightly}
    container_name: mindfu-llm-vllm
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - LD_LIBRARY_PATH=/lib/x86_64-linux-gnu:/usr/local/cuda/lib64
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-0}
      - VLLM_ATTENTION_BACKEND=${VLLM_ATTENTION_BACKEND:-FLASHINFER}
    dns:
      - 8.8.8.8
      - 1.1.1.1
    volumes:
      - model-cache:/root/.cache/huggingface
      # PATCH: Fix JSONDecodeError on malformed tool call arguments
      - ./configs/vllm/entrypoint-patch.sh:/entrypoint-patch.sh:ro
    ports:
      - "8000:8000"
    entrypoint: ["/bin/bash", "/entrypoint-patch.sh"]
    command: >
      --model ${VLLM_MODEL:-mistralai/Devstral-Small-2-24B-Instruct-2512}
      --host 0.0.0.0
      --port 8000
      --dtype auto
      --max-model-len ${VLLM_MAX_MODEL_LEN:-81920}
      --gpu-memory-utilization ${VLLM_GPU_MEM:-0.95}
      --kv-cache-dtype ${VLLM_KV_CACHE_DTYPE:-fp8}
      --tool-call-parser mistral
      --enable-auto-tool-choice
      --enable-prefix-caching
      --enable-chunked-prefill
      --async-scheduling
      --tensor-parallel-size ${VLLM_TP_SIZE:-1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 600s  # 10 min - vLLM needs time to load 24B model
    profiles:
      - vllm
    networks:
      - mindfu-network
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # RAG Service for vLLM backend
  # ---------------------------------------------------------------------------
  rag-vllm:
    image: ghcr.io/n0cl0ud/mindfu-rag:latest
    container_name: mindfu-rag-vllm
    environment:
      - LLM_BASE_URL=http://llm-vllm:8000/v1
      - LLM_MODEL=${VLLM_MODEL:-mistralai/Devstral-Small-2-24B-Instruct-2512}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-mindfu}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mindfu_secret}
      - POSTGRES_DB=${POSTGRES_DB:-mindfu}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      - DEFAULT_COLLECTION=${DEFAULT_COLLECTION:-documents}
      - LOG_CONVERSATIONS=${LOG_CONVERSATIONS:-false}
      # Streaming with tools broken in vLLM Mistral parser (IndexError in streamed_args_for_tool)
      - FORCE_NO_STREAM_WITH_TOOLS=true
    volumes:
      - ./data/documents:/app/documents
      - ./data/conversations:/app/conversations
      - embedding-cache:/root/.cache/huggingface
    ports:
      - "11434:8080"
    depends_on:
      qdrant:
        condition: service_started
      redis:
        condition: service_healthy
      llm-vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    profiles:
      - vllm
    networks:
      - mindfu-network
    restart: unless-stopped

  # =============================================================================
  # NEMOTRON STACK (profile: nemotron) - Nemotron 3 Nano with 1M context
  # =============================================================================

  # ---------------------------------------------------------------------------
  # LLM Inference - vLLM with Nemotron 3 Nano (30B params, 3B active MoE)
  # ---------------------------------------------------------------------------
  llm-nemotron:
    image: vllm/vllm-openai:${VLLM_VERSION:-nightly}
    container_name: mindfu-llm-nemotron
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - LD_LIBRARY_PATH=/lib/x86_64-linux-gnu:/usr/local/cuda/lib64
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - model-cache:/root/.cache/huggingface
      # PATCH: Fix JSONDecodeError on malformed tool call arguments
      - ./configs/vllm/entrypoint-patch.sh:/entrypoint-patch.sh:ro
    ports:
      - "8000:8000"
    entrypoint: ["/bin/bash", "/entrypoint-patch.sh"]
    command: >
      --model nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8
      --host 0.0.0.0
      --port 8000
      --max-model-len 65536
      --max-num-seqs 3
      --gpu-memory-utilization 0.90
      --trust-remote-code
      --kv-cache-dtype fp8
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
      --reasoning-parser deepseek_r1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 900s  # 15 min - Nemotron needs time to load
    profiles:
      - nemotron
    networks:
      - mindfu-network
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # RAG Service for Nemotron backend
  # ---------------------------------------------------------------------------
  rag-nemotron:
    image: ghcr.io/n0cl0ud/mindfu-rag:latest
    container_name: mindfu-rag-nemotron
    environment:
      - LLM_BASE_URL=http://llm-nemotron:8000/v1
      - LLM_MODEL=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-mindfu}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mindfu_secret}
      - POSTGRES_DB=${POSTGRES_DB:-mindfu}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      - DEFAULT_COLLECTION=${DEFAULT_COLLECTION:-documents}
      - LOG_CONVERSATIONS=${LOG_CONVERSATIONS:-false}
      # Nemotron uses qwen3_coder parser which supports streaming tool calls
      - FORCE_NO_STREAM_WITH_TOOLS=false
    volumes:
      - ./data/documents:/app/documents
      - ./data/conversations:/app/conversations
      - embedding-cache:/root/.cache/huggingface
    ports:
      - "11434:8080"
    depends_on:
      qdrant:
        condition: service_started
      redis:
        condition: service_healthy
      llm-nemotron:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    profiles:
      - nemotron
    networks:
      - mindfu-network
    restart: unless-stopped

  # =============================================================================
  # TRAINING SERVICE (profile: training)
  # =============================================================================
  training:
    image: ghcr.io/n0cl0ud/mindfu-training:latest
    container_name: mindfu-training
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - RUN_API=true
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-mindfu}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mindfu_secret}
      - POSTGRES_DB=${POSTGRES_DB:-mindfu}
      - BASE_MODEL=${TRAINING_MODEL:-mistralai/Devstral-Small-2-24B-Instruct-2512}
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - model-cache:/root/.cache/huggingface
      - ./data/models:/models
      - ./data/conversations:/conversations
    ports:
      - "5001:5001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      postgres:
        condition: service_healthy
      mlflow:
        condition: service_started
    profiles:
      - training
    networks:
      - mindfu-network
    restart: "no"

volumes:
  model-cache:
  qdrant-data:
  postgres-data:
  redis-data:
  mlflow-artifacts:
  embedding-cache:

networks:
  mindfu-network:
    driver: bridge
